{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "398e28b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.24\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f614cf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dotenv loaded: True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# Adjust path if notebook isn't in 'notebook/' dir relative to root .env\n",
    "loaded = load_dotenv(\"../.env\")\n",
    "print(f\"Dotenv loaded: {loaded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4344bcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "(wait for it...)\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that made you smile!\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Initialize LLM (needs GROQ_API_KEY)\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\") # Or your chosen model/provider\n",
    "\n",
    "# Send a message directly\n",
    "response = llm.invoke([HumanMessage(content=\"Tell me a joke\")])\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95c602c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here's one:\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\n(wait for it...)\\n\\nBecause it was two-tired!\\n\\nHope that made you smile!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 14, 'total_tokens': 46, 'completion_time': 0.026666667, 'prompt_time': 0.002999473, 'queue_time': 0.235684042, 'total_time': 0.02966614}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_dadc9d6142', 'finish_reason': 'stop', 'logprobs': None}, id='run-a1aa3111-9147-421c-aea8-f646faddfc7c-0', usage_metadata={'input_tokens': 14, 'output_tokens': 32, 'total_tokens': 46})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d00f591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "(wait for it...)\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that made you smile!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "print(output_parser.invoke(response)) # Parses the AIMessage object's content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "609d340d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "(wait for it...)\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that made you smile!\n"
     ]
    }
   ],
   "source": [
    "# Chain: Input -> LLM -> Parse Output to String\n",
    "basic_chain = llm | output_parser\n",
    "\n",
    "llm_response = basic_chain.invoke(\"Tell me a joke!\") # Input is passed to llm\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c38d7f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='Tell me a short joke about programming', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Template with a placeholder {topic}\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "\n",
    "# See the prompt structure\n",
    "print(prompt.invoke({\"topic\": \"programming\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "889948c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why did the programmer quit his job?\n",
      "\n",
      "Because he didn't get arrays! (get a raise)\n"
     ]
    }
   ],
   "source": [
    "# Chain the prompt template with the basic LLM chain\n",
    "joke_chain = prompt | llm | output_parser\n",
    "\n",
    "# Invoke with a specific topic\n",
    "programmer_joke = joke_chain.invoke({\"topic\": \"programmer\"})\n",
    "print(programmer_joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c8ceffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct invoke with persona:\n",
      " Programming! It's like trying to solve a puzzle blindfolded while being attacked by a swarm of bees. Just kidding, it's actually really fun once you get the hang of it!\n",
      "\n",
      "But seriously, programming is like writing a recipe for your computer. You take some ingredients (like variables and functions), mix them together in a certain way (using syntax and logic), and voila! You get a delicious program that can do all sorts of cool things.\n",
      "\n",
      "Speaking of cool things, did you hear about the programmer who quit his job because he didn't get arrays? (get it? arrays? ahaha)\n",
      "\n",
      "Okay, okay, I'll stop with the jokes for now. But seriously, programming is a great skill to have, and there are so many resources available to help you learn. Whether you're interested in web development, game development, or just want to automate some tasks, there's a programming language out there for you.\n",
      "\n",
      "So, what kind of programming are you interested in? I can give you some resources and tips to get you started!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "system_message = SystemMessage(content=\"You are a helpful assistant that tells jokes.\")\n",
    "human_message = HumanMessage(content=\"Tell me about programming\")\n",
    "\n",
    "# Direct invocation with system/human message list\n",
    "response_persona = llm.invoke([system_message, human_message])\n",
    "print(\"Direct invoke with persona:\\n\", response_persona.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e480b7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant that tells jokes.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about programming', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# More common: Use ChatPromptTemplate with roles\n",
    "template_persona = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that tells jokes.\"),\n",
    "        (\"human\", \"Tell me about {topic}\")\n",
    "    ]\n",
    ")\n",
    "# See the structured prompt\n",
    "prompt_value_persona = template_persona.invoke({\"topic\": \"programming\"})\n",
    "print(prompt_value_persona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b3b5873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chain invoke with persona:\n",
      "Programming! It's a real \"byte\"-sized challenge, isn't it? But don't worry, I'm here to help you \"debug\" your understanding!\n",
      "\n",
      "Programming is like cooking a recipe. You need to follow the steps, add the right ingredients, and stir it up just right to get the desired result. But instead of flour, sugar, and eggs, you're working with code, algorithms, and data structures!\n",
      "\n",
      "Here's a joke to \"compile\" your thoughts:\n",
      "\n",
      "Why do programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs!\n"
     ]
    }
   ],
   "source": [
    "# Chain with the persona template\n",
    "persona_chain = template_persona | llm | output_parser\n",
    "response_persona_chain = persona_chain.invoke({\"topic\": \"programming\"})\n",
    "print(\"\\nChain invoke with persona:\")\n",
    "print(response_persona_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a9a7f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured Output Object: phone_model='Galaxy S21' rating=4.0 pros=['gorgeous screen', 'colors pop', 'insane camera'] cons=['pricey', 'no charger included', 'new button layout takes getting used to'] summary='Solid phone, but a few annoying quirks keep it from being perfect'\n",
      "\n",
      "Pros: ['gorgeous screen', 'colors pop', 'insane camera']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define the desired output structure\n",
    "class MobileReview(BaseModel):\n",
    "    phone_model: str = Field(description=\"Name and model of the phone\")\n",
    "    rating: float = Field(description=\"Overall rating out of 5\")\n",
    "    pros: List[str] = Field(description=\"List of positive aspects\")\n",
    "    cons: List[str] = Field(description=\"List of negative aspects\")\n",
    "    summary: str = Field(description=\"Brief summary of the review\")\n",
    "\n",
    "review_text = \"\"\"\n",
    "Just got my hands on the new Galaxy S21 and wow, this thing is slick! The screen is gorgeous,\n",
    "colors pop like crazy. Camera's insane too, especially at night - my Insta game's never been\n",
    "stronger. Battery life's solid, lasts me all day no problem.\n",
    "Not gonna lie though, it's pretty pricey. And what's with ditching the charger? C'mon Samsung.\n",
    "Also, still getting used to the new button layout, keep hitting Bixby by mistake.\n",
    "Overall, I'd say it's a solid 4 out of 5. Great phone, but a few annoying quirks keep it from\n",
    "being perfect. If you're due for an upgrade, definitely worth checking out!\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Create an LLM variant that outputs in the MobileReview format\n",
    "    structured_llm = llm.with_structured_output(MobileReview)\n",
    "    output_obj = structured_llm.invoke(review_text)\n",
    "    print(\"Structured Output Object:\", output_obj)\n",
    "    print(\"\\nPros:\", output_obj.pros)\n",
    "except Exception as e:\n",
    "    print(f\"Structured output failed: {e}\") # Might depend on LLM's ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a171dddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from: data/\n",
      "    Loaded company_profile.pdf (1 parts).\n",
      "    Loaded employee_handbook.pdf (1 parts).\n",
      "    Loaded it_support_policy.pdf (1 parts).\n",
      "    Loaded meeting_guidelines.pdf (1 parts).\n",
      "    Loaded product_faq.pdf (1 parts).\n",
      "Loaded 5 documents total.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "\n",
    "def load_documents(folder_path: str) -> List[Document]:\n",
    "    documents = []\n",
    "    print(f\"Loading documents from: {folder_path}\")\n",
    "    if not os.path.isdir(folder_path): return []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        loader = None\n",
    "        if filename.endswith('.pdf'): loader = PyPDFLoader(file_path)\n",
    "        elif filename.endswith('.docx'): loader = Docx2txtLoader(file_path)\n",
    "        else: print(f\"  Skipping unsupported file type: {filename}\"); continue\n",
    "        try:\n",
    "            loaded_docs = loader.load(); documents.extend(loaded_docs)\n",
    "            print(f\"    Loaded {filename} ({len(loaded_docs)} parts).\")\n",
    "        except Exception as e: print(f\"    Error loading {filename}: {e}\")\n",
    "    return documents\n",
    "\n",
    "# Adjust path as needed\n",
    "folder_path = \"data/\"\n",
    "documents = load_documents(folder_path)\n",
    "print(f\"Loaded {len(documents)} documents total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "846848bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the documents into 5 chunks.\n",
      "\n",
      "Original Document 0 Metadata: {'producer': 'PyFPDF 1.7.2 http://pyfpdf.googlecode.com/', 'creator': 'PyPDF', 'creationdate': 'D:20250418155508', 'source': 'data/company_profile.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "First Split Chunk Metadata: {'producer': 'PyFPDF 1.7.2 http://pyfpdf.googlecode.com/', 'creator': 'PyPDF', 'creationdate': 'D:20250418155508', 'source': 'data/company_profile.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "First Split Chunk Content: Company Profile\n",
      "Company Name: FutureTech Corp\n",
      "Founded: 2012\n",
      "Headquarters: San Francisco, CA\n",
      "Employees: 1,200+\n",
      "Mission:\n",
      "To innovate intelligent solutions that simplify lives and empower industries through AI\n",
      "and automation.\n",
      "Flagship Products:\n",
      "- Vision360 AI Camera\n",
      "- AutoInsights Business Dashboard\n",
      "- ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, length_function=len\n",
    ")\n",
    "\n",
    "if documents: # Only split if documents were loaded\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    print(f\"Split the documents into {len(splits)} chunks.\")\n",
    "else:\n",
    "    splits = []\n",
    "    print(\"No documents to split.\")\n",
    "\n",
    "if documents: print(\"\\nOriginal Document 0 Metadata:\", documents[0].metadata)\n",
    "if splits: print(\"\\nFirst Split Chunk Metadata:\", splits[0].metadata)\n",
    "if splits: print(\"\\nFirst Split Chunk Content:\", splits[0].page_content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc8b329b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing NOMIC Embeddings (requires NOMIC_API_KEY)...\n",
      "Nomic Embeddings Initialized.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: This uses Nomic Embeddings, requires NOMIC_API_KEY\n",
    "from langchain_nomic import NomicEmbeddings\n",
    "\n",
    "embedding_function_nomic = None # Initialize\n",
    "if splits:\n",
    "    try:\n",
    "        print(\"Initializing NOMIC Embeddings (requires NOMIC_API_KEY)...\")\n",
    "        embeddings_nomic = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\n",
    "        print(\"Nomic Embeddings Initialized.\")\n",
    "        # Example embedding one chunk\n",
    "        # first_chunk_embedding = embeddings_nomic.embed_documents([splits[0].page_content])\n",
    "        # print(f\"Example vector length: {len(first_chunk_embedding[0])}\")\n",
    "        embedding_function_nomic = embeddings_nomic # Store for use\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR initializing Nomic Embeddings: {e}\")\n",
    "else:\n",
    "    print(\"Skipping embeddings: No chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77cb08bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to create/load Chroma DB at ./chroma_db_research_nomic...\n",
      "Chroma vector store ready using Nomic Embeddings.\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectorstore = None # Initialize\n",
    "if splits and embedding_function_nomic: # Check if prerequisites met\n",
    "    try:\n",
    "        collection_name = \"research_docs_nomic\"\n",
    "        persist_directory = \"./chroma_db_research_nomic\" # Local path\n",
    "        print(f\"Attempting to create/load Chroma DB at {persist_directory}...\")\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            collection_name=collection_name,\n",
    "            documents=splits,\n",
    "            embedding=embedding_function_nomic, # Use the Nomic embeddings here\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        print(\"Chroma vector store ready using Nomic Embeddings.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR creating/loading Chroma DB: {e}\")\n",
    "        vectorstore = None\n",
    "else:\n",
    "    print(\"Skipping Chroma DB creation (missing splits or Nomic embedding function).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da0584b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing similarity search for: 'When was FutureTech Corp founded?'\n",
      "\n",
      "Top 2 search results:\n",
      "--- Result 1 ---\n",
      "Source: data/company_profile.pdf\n",
      "Content: Company Profile\n",
      "Company Name: FutureTech Corp\n",
      "Founded: 2012\n",
      "Headquarters: San Francisco, CA\n",
      "Employees: 1,200+\n",
      "Mission:\n",
      "To innovate intelligent solutions that simplify lives and empower industries thro...\n",
      "---------------\n",
      "--- Result 2 ---\n",
      "Source: data/employee_handbook.pdf\n",
      "Content: Employee Handbook\n",
      "Welcome to FutureTech Corp!\n",
      "Working Hours:\n",
      "- Standard hours are 9:00 AM to 6:00 PM, Monday to Friday.\n",
      "- Employees may request flexible hours subject to manager approval.\n",
      "Leave Policy...\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "if vectorstore:\n",
    "    query = \"When was FutureTech Corp founded?\"\n",
    "    try:\n",
    "        print(f\"\\nPerforming similarity search for: '{query}'\")\n",
    "        search_results = vectorstore.similarity_search(query, k=2) # Get top 2\n",
    "        print(\"\\nTop 2 search results:\")\n",
    "        for i, result in enumerate(search_results, 1):\n",
    "            print(f\"--- Result {i} ---\")\n",
    "            print(f\"Source: {result.metadata.get('source', 'Unknown')}\")\n",
    "            print(f\"Content: {result.page_content[:200]}...\") # Show snippet\n",
    "            print(\"-\" * 15)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during similarity search: {e}\")\n",
    "else:\n",
    "    print(\"Skipping similarity search: No vector store.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4be11a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retriever created.\n"
     ]
    }
   ],
   "source": [
    "retriever = None\n",
    "if vectorstore:\n",
    "    try:\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "        print(\"\\nRetriever created.\")\n",
    "        # retrieved_docs = retriever.invoke(query) # Example invocation\n",
    "        # print(f\"Retriever found {len(retrieved_docs)} docs.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating retriever: {e}\")\n",
    "        retriever = None\n",
    "else:\n",
    "    print(\"Skipping retriever creation: No vector store.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06f991fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building basic RAG chain...\n",
      "Basic RAG chain created.\n",
      "\n",
      "Invoking basic RAG chain for: 'When was FutureTech Corp founded?'\n",
      "\n",
      "Question: When was FutureTech Corp founded?\n",
      "Answer: 2012\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "if retriever and llm: # Check if prerequisites exist\n",
    "    print(\"\\nBuilding basic RAG chain...\")\n",
    "    template_rag = \"\"\"Answer the question based only on the following context:\n",
    "    {context}   \n",
    "    Question: {question}\n",
    "    Answer: \"\"\"\n",
    "\n",
    "    prompt_rag = ChatPromptTemplate.from_template(template_rag)\n",
    "\n",
    "    def format_docs(docs): # Helper function\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    rag_chain_basic = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt_rag\n",
    "        | llm\n",
    "        | output_parser # Use the StrOutputParser from earlier\n",
    "    )\n",
    "    \n",
    "    print(\"Basic RAG chain created.\")\n",
    "    \n",
    "    try:\n",
    "        question_rag = \"When was FutureTech Corp founded?\"\n",
    "        print(f\"\\nInvoking basic RAG chain for: '{question_rag}'\")\n",
    "        response_basic_rag = rag_chain_basic.invoke(question_rag)\n",
    "        print(f\"\\nQuestion: {question_rag}\")\n",
    "        print(f\"Answer: {response_basic_rag}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking basic RAG chain: {e}\")\n",
    "else:\n",
    "    print(\"Skipping basic RAG chain creation (missing retriever or llm).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "263553a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated chat history (list format):\n",
      "[HumanMessage(content='When was FutureTech Corp founded?', additional_kwargs={}, response_metadata={}), AIMessage(content='2012', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# Simple list to simulate history for the notebook run\n",
    "notebook_chat_history = []\n",
    "question1_hist = \"When was FutureTech Corp founded?\"\n",
    "# Assume 'response_basic_rag' holds the answer from the previous step\n",
    "# In a real app, you'd use the RAG chain with history handling built-in\n",
    "answer1_hist = response_basic_rag if 'response_basic_rag' in locals() else \"FutureTech Corp was founded in 2012.\"\n",
    "\n",
    "notebook_chat_history.extend([\n",
    "    HumanMessage(content=question1_hist),\n",
    "    AIMessage(content=answer1_hist)\n",
    "])\n",
    "print(\"Simulated chat history (list format):\")\n",
    "print(notebook_chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41f8c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "DB_NAME = \"rag_app.db\"\n",
    "\n",
    "def get_db_connection():\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    conn.row_factory = sqlite3.Row\n",
    "    return conn\n",
    "\n",
    "def create_application_logs():\n",
    "    conn = get_db_connection()\n",
    "    conn.execute('''CREATE TABLE IF NOT EXISTS application_logs\n",
    "    (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    session_id TEXT,\n",
    "    user_query TEXT,\n",
    "    gpt_response TEXT,\n",
    "    model TEXT,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')\n",
    "    conn.close()\n",
    "\n",
    "def insert_application_logs(session_id, user_query, gpt_response, model):\n",
    "    conn = get_db_connection()\n",
    "    conn.execute('INSERT INTO application_logs (session_id, user_query, gpt_response, model) VALUES (?, ?, ?, ?)',\n",
    "                 (session_id, user_query, gpt_response, model))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def get_chat_history(session_id):\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT user_query, gpt_response FROM application_logs WHERE session_id = ? ORDER BY created_at', (session_id,))\n",
    "    messages = []\n",
    "    for row in cursor.fetchall():\n",
    "        messages.extend([\n",
    "            {\"role\": \"human\", \"content\": row['user_query']},\n",
    "            {\"role\": \"ai\", \"content\": row['gpt_response']}\n",
    "        ])\n",
    "    conn.close()\n",
    "    return messages\n",
    "\n",
    "# Initialize the database\n",
    "create_application_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0244351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Storing/Retrieving History with DB ---\n",
      "Using session_id: 52366755-336a-4ce9-a7b1-6300815eeb85\n",
      "Logged Turn 1 to DB.\n",
      "Logged Turn 2 to DB.\n",
      "\n",
      "History retrieved from DB:\n",
      "[{'role': 'human', 'content': 'What is FutureTech Corp?'}, {'role': 'ai', 'content': 'FutureTech Corp, founded in 2012 and based in SF, creates AI solutions like Vision360.'}, {'role': 'human', 'content': 'What are their flagship product?'}, {'role': 'ai', 'content': 'Their main products are Vision360 AI Camera, AutoInsights Dashboard, and RoboHR Assistant.'}]\n"
     ]
    }
   ],
   "source": [
    "# Using the database functions defined earlier\n",
    "print(\"\\n--- Storing/Retrieving History with DB ---\")\n",
    "try:\n",
    "    session_id_db = str(uuid.uuid4())\n",
    "    print(f\"Using session_id: {session_id_db}\")\n",
    "\n",
    "    # Simulate turn 1 from notebook cell 50\n",
    "    q1_db = \"What is FutureTech Corp?\"\n",
    "    # We need to run the full RAG chain again for a proper answer\n",
    "    # For demonstration, let's use a placeholder answer\n",
    "    a1_db = \"FutureTech Corp, founded in 2012 and based in SF, creates AI solutions like Vision360.\"\n",
    "    insert_application_logs(session_id_db, q1_db, a1_db, \"llama3-8b-8192\")\n",
    "    print(f\"Logged Turn 1 to DB.\")\n",
    "\n",
    "    # Simulate turn 2 from notebook cell 50\n",
    "    q2_db = \"What are their flagship product?\"\n",
    "    a2_db = \"Their main products are Vision360 AI Camera, AutoInsights Dashboard, and RoboHR Assistant.\"\n",
    "    insert_application_logs(session_id_db, q2_db, a2_db, \"llama3-8b-8192\")\n",
    "    print(f\"Logged Turn 2 to DB.\")\n",
    "\n",
    "    # Retrieve history from DB (like notebook cell 52)\n",
    "    retrieved_db_history = get_chat_history(session_id_db)\n",
    "    print(\"\\nHistory retrieved from DB:\")\n",
    "    print(retrieved_db_history)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error interacting with chat history DB: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "918570fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Building History-Aware RAG Chain ---\n",
      "History-aware retriever created.\n",
      "Question-answer chain created.\n",
      "Full history-aware RAG chain created.\n",
      "\n",
      "Reformulated question based on history: What is the headquarters location of FutureTech Corp?\n",
      "\n",
      "Invoking full RAG chain with history...\n",
      "\n",
      "Human: Where is it headquartered?\n",
      "AI: San Francisco, CA\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "if retriever and llm: # Check prerequisites\n",
    "    print(\"\\n--- Building History-Aware RAG Chain ---\")\n",
    "    # 1. Contextualizer Prompt: Reformulates question based on history\n",
    "    contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"), # Where history is injected\n",
    "        (\"human\", \"{input}\"),\n",
    "    ])\n",
    "\n",
    "    # 2. History-Aware Retriever: Runs contextualizer then retrieves\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "    print(\"History-aware retriever created.\")\n",
    "\n",
    "    # 3. QA Prompt: Includes context AND history for final answer\n",
    "    qa_prompt_hist = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful AI assistant. Use the following context to answer the user's question.\"),\n",
    "        (\"system\", \"Context: {context}\"), # Retrieved docs go here\n",
    "        MessagesPlaceholder(\"chat_history\"), # History goes here\n",
    "        (\"human\", \"{input}\") # Standalone question goes here\n",
    "    ])\n",
    "\n",
    "    # 4. QA Chain: Takes context/history/question -> generates answer\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt_hist)\n",
    "    print(\"Question-answer chain created.\")\n",
    "\n",
    "    # 5. Final RAG Chain: Connects history_aware_retriever and question_answer_chain\n",
    "    rag_chain_with_history = create_retrieval_chain(\n",
    "        history_aware_retriever, question_answer_chain\n",
    "    )\n",
    "    print(\"Full history-aware RAG chain created.\")\n",
    "\n",
    "    try:\n",
    "         # Example: See how the question gets reformulated\n",
    "         reformulated_q = (contextualize_q_prompt | llm | StrOutputParser()).invoke(\n",
    "             {\"input\": \"Where is it headquartered?\", \"chat_history\": notebook_chat_history}\n",
    "         )\n",
    "         print(f\"\\nReformulated question based on history: {reformulated_q}\")\n",
    "\n",
    "         # Example: See what docs the history-aware retriever fetches\n",
    "         # relevant_docs_hist = history_aware_retriever.invoke(\n",
    "         #     {\"input\": \"Where is it headquartered?\", \"chat_history\": notebook_chat_history}\n",
    "         # )\n",
    "         # print(f\"\\nDocs retrieved for reformulated question: {len(relevant_docs_hist)} docs\")\n",
    "    except Exception as e:\n",
    "         print(f\"Error demonstrating intermediate steps: {e}\")\n",
    "\n",
    "    try:\n",
    "        print(\"\\nInvoking full RAG chain with history...\")\n",
    "        question_follow_up = \"Where is it headquartered?\"\n",
    "        answer_follow_up = rag_chain_with_history.invoke(\n",
    "            {\"input\": question_follow_up, \"chat_history\": notebook_chat_history}\n",
    "        )['answer']\n",
    "\n",
    "        print(f\"\\nHuman: {question_follow_up}\")\n",
    "        print(f\"AI: {answer_follow_up}\")\n",
    "        # Add to notebook history list\n",
    "        # notebook_chat_history.extend([HumanMessage(content=question_follow_up), AIMessage(content=answer_follow_up)])\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking history-aware RAG chain: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping history-aware RAG chain (missing retriever or llm).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
